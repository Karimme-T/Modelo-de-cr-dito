{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Credit Scoring con Deep Learning\n",
        "## Red Neuronal para Predicci\u00f3n de Incumplimiento de Tarjetas de Cr\u00e9dito\n",
        "\n",
        "Este notebook implementa un modelo de Deep Learning para calcular el score crediticio utilizando una red neuronal artificial (ANN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## 1. Importar Librer\u00edas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {
        "id": "imports_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## 2. Cargar y Explorar los Datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset\n",
        "# NOTA: Ajusta la ruta seg\u00fan la ubicaci\u00f3n de tu archivo\n",
        "df = pd.read_csv('/content/drive/MyDrive/Credit Model/Credit_Card_Dataset.csv')\n",
        "\n",
        "print(\"Dimensiones del dataset:\", df.shape)\n",
        "print(\"\\nPrimeras filas:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "load_data_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Informaci\u00f3n general del dataset\n",
        "print(\"\\nInformaci\u00f3n del dataset:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nEstad\u00edsticas descriptivas:\")\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "info_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar valores nulos\n",
        "print(\"\\nValores nulos por columna:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Distribuci\u00f3n de la variable objetivo\n",
        "print(\"\\nDistribuci\u00f3n de la variable 'Defaulted':\")\n",
        "print(df['Defaulted'].value_counts())\n",
        "print(f\"\\nPorcentaje de incumplimiento: {df['Defaulted'].mean()*100:.2f}%\")"
      ],
      "metadata": {
        "id": "check_null_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 3. Preprocesamiento de Datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una copia del dataframe para preprocesamiento\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Eliminar Customer_ID ya que es un identificador \u00fanico\n",
        "df_processed = df_processed.drop('Customer_ID', axis=1)\n",
        "\n",
        "print(\"Columnas del dataset:\", df_processed.columns.tolist())\n",
        "print(f\"\\nN\u00famero de caracter\u00edsticas: {df_processed.shape[1]-1}\")"
      ],
      "metadata": {
        "id": "drop_id_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificar columnas categ\u00f3ricas y num\u00e9ricas\n",
        "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Remover 'Defaulted' de las columnas num\u00e9ricas (es la variable objetivo)\n",
        "if 'Defaulted' in numerical_cols:\n",
        "    numerical_cols.remove('Defaulted')\n",
        "\n",
        "print(f\"Columnas categ\u00f3ricas ({len(categorical_cols)}): {categorical_cols}\")\n",
        "print(f\"\\nColumnas num\u00e9ricas ({len(numerical_cols)}): {numerical_cols}\")"
      ],
      "metadata": {
        "id": "identify_cols_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar variables categ\u00f3ricas usando Label Encoding\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_processed[col] = le.fit_transform(df_processed[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"{col}: {len(le.classes_)} categor\u00edas\")\n",
        "\n",
        "print(\"\\nCodificaci\u00f3n completada.\")"
      ],
      "metadata": {
        "id": "encode_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar caracter\u00edsticas (X) y variable objetivo (y)\n",
        "X = df_processed.drop('Defaulted', axis=1)\n",
        "y = df_processed['Defaulted']\n",
        "\n",
        "print(f\"Forma de X: {X.shape}\")\n",
        "print(f\"Forma de y: {y.shape}\")\n",
        "print(f\"\\nN\u00famero total de caracter\u00edsticas: {X.shape[1]}\")"
      ],
      "metadata": {
        "id": "split_xy_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir en conjuntos de entrenamiento y prueba (80-20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Tama\u00f1o del conjunto de entrenamiento: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Tama\u00f1o del conjunto de prueba: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nDistribuci\u00f3n en entrenamiento - Incumplimiento: {y_train.mean()*100:.2f}%\")\n",
        "print(f\"Distribuci\u00f3n en prueba - Incumplimiento: {y_test.mean()*100:.2f}%\")"
      ],
      "metadata": {
        "id": "train_test_split_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar el scaler seg\u00fan hiperpar\u00e1metros",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler",
        "",
        "if hiperparametros['scaler_type'] == 'standard':",
        "    scaler = StandardScaler()",
        "elif hiperparametros['scaler_type'] == 'minmax':",
        "    scaler = MinMaxScaler(feature_range=hiperparametros['scaler_feature_range'])",
        "elif hiperparametros['scaler_type'] == 'robust':",
        "    scaler = RobustScaler()",
        "elif hiperparametros['scaler_type'] == 'maxabs':",
        "    scaler = MaxAbsScaler()",
        "else:",
        "    scaler = StandardScaler()",
        "",
        "X_train_scaled = scaler.fit_transform(X_train)",
        "X_test_scaled = scaler.transform(X_test)",
        "",
        "print(f\"Escalado completado usando: {hiperparametros['scaler_type']}\")",
        "print(f\"\\nMedia de caracter\u00edsticas (train): {X_train_scaled.mean():.6f}\")",
        "print(f\"Desviaci\u00f3n est\u00e1ndar (train): {X_train_scaled.std():.6f}\")"
      ],
      "metadata": {
        "id": "scale_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_architecture"
      },
      "source": [
        "## 4. Construcci\u00f3n del Modelo de Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los hiperpar\u00e1metros",
        "hiperparametros = {",
        "    'num_layers': 6,  # N\u00famero total de capas ocultas",
        "    'neurons': [500, 200, 100, 16],  # Neuronas por capa (se repite el \u00faltimo si faltan)",
        "    'activation_hidden': 'relu',  ",
        "    'activation_output': 'sigmoid',  ",
        "    'kernel_initializer': 'he_uniform',  ",
        "    ",
        "    'dropout_rates': [0.3, 0.3, 0.2, 0.2, 0.2, 0.2],  # Dropout por capa",
        "    'use_batch_norm': True,  ",
        "    'batch_norm_momentum': 0.99,  ",
        "    'batch_norm_epsilon': 0.001,  ",
        "    'l1_regularization': 0.0,  # L1 regularization (0.0 = desactivado)",
        "    'l2_regularization': 0.01,  # L2 regularization (0.0 = desactivado)",
        "    'kernel_constraint': None,  # None o valor para max_norm",
        "    ",
        "    'optimizer_type': 'adam',  ",
        "    'learning_rate': 0.001,  ",
        "    'beta_1': 0.9,  # Para Adam/Nadam",
        "    'beta_2': 0.999,  # Para Adam/Nadam",
        "    'epsilon': 1e-07,  # Para Adam",
        "    'amsgrad': False,  # Usar AMSGrad variant",
        "    'momentum': 0.0,  # Para SGD (si se usa)",
        "    'nesterov': False,  # Usar Nesterov momentum (para SGD)",
        "    'clipnorm': None,  # Gradient clipping por norma (None = desactivado)",
        "    'clipvalue': None,  # Gradient clipping por valor (None = desactivado)",
        "    ",
        "    'loss': 'binary_crossentropy',  ",
        "    'focal_loss_gamma': 2.0,  ",
        "    'focal_loss_alpha': 0.25, ",
        "    ",
        "    'metrics': ['accuracy', 'auc', 'precision', 'recall'], ",
        "    ",
        "    'epochs': 300,  ",
        "    'batch_size': 50, ",
        "    'shuffle': True,  # Mezclar datos en cada \u00e9poca",
        "    ",
        "    'use_early_stopping': True,",
        "    'early_stopping_monitor': 'val_loss',  # 'val_loss', 'val_accuracy', 'val_auc'",
        "    'early_stopping_patience': 15,",
        "    'early_stopping_min_delta': 0.0001,",
        "    'early_stopping_mode': 'auto',  ",
        "    'restore_best_weights': True,",
        "    ",
        "    'use_reduce_lr': True,",
        "    'reduce_lr_monitor': 'val_loss',",
        "    'reduce_lr_factor': 0.5,",
        "    'reduce_lr_patience': 5,",
        "    'reduce_lr_min_lr': 0.00001,",
        "    'reduce_lr_min_delta': 0.0001,",
        "",
        "    'scaler_type': 'standard',  # 'standard', 'minmax', 'robust', 'maxabs'",
        "    'scaler_feature_range': (0, 1),  # Para MinMaxScaler",
        "    ",
        "    'verbose': 1,  ",
        "    'prediction_threshold': 0.5, ",
        "}",
        "",
        "# Definir la arquitectura de la red neuronal con hiperpar\u00e1metros configurables",
        "def create_deep_learning_model(input_dim, hp):",
        "    \"\"\"",
        "    Crea un modelo de red neuronal profunda para clasificaci\u00f3n binaria.",
        "    ",
        "    Args:",
        "        input_dim: Dimensi\u00f3n de entrada (n\u00famero de caracter\u00edsticas)",
        "        hp: Diccionario con hiperpar\u00e1metros",
        "    ",
        "    Returns:",
        "        Modelo compilado de Keras",
        "    \"\"\"",
        "    from tensorflow.keras import regularizers",
        "    from tensorflow.keras.constraints import MaxNorm",
        "    ",
        "    model = Sequential()",
        "    ",
        "    # Extender la lista de neuronas si es necesario",
        "    neurons_list = hp['neurons'].copy()",
        "    if len(neurons_list) < hp['num_layers']:",
        "        last_neurons = neurons_list[-1]",
        "        neurons_list.extend([last_neurons] * (hp['num_layers'] - len(neurons_list)))",
        "    ",
        "    # Extender la lista de dropout si es necesario",
        "    dropout_list = hp['dropout_rates'].copy()",
        "    if len(dropout_list) < hp['num_layers']:",
        "        dropout_list.extend([dropout_list[-1]] * (hp['num_layers'] - len(dropout_list)))",
        "    ",
        "    # Configurar regularizaci\u00f3n",
        "    kernel_reg = None",
        "    if hp['l1_regularization'] > 0 and hp['l2_regularization'] > 0:",
        "        kernel_reg = regularizers.l1_l2(l1=hp['l1_regularization'], l2=hp['l2_regularization'])",
        "    elif hp['l1_regularization'] > 0:",
        "        kernel_reg = regularizers.l1(hp['l1_regularization'])",
        "    elif hp['l2_regularization'] > 0:",
        "        kernel_reg = regularizers.l2(hp['l2_regularization'])",
        "    ",
        "    # Configurar constraint",
        "    kernel_const = MaxNorm(hp['kernel_constraint']) if hp['kernel_constraint'] else None",
        "    ",
        "    # Crear capas ocultas",
        "    for i in range(hp['num_layers']):",
        "        if i == 0:",
        "            # Primera capa (entrada)",
        "            model.add(Dense(",
        "                neurons_list[i],",
        "                activation=hp['activation_hidden'],",
        "                input_dim=input_dim,",
        "                kernel_initializer=hp['kernel_initializer'],",
        "                kernel_regularizer=kernel_reg,",
        "                kernel_constraint=kernel_const,",
        "                name=f'layer_{i}'",
        "            ))",
        "        else:",
        "            # Capas ocultas",
        "            model.add(Dense(",
        "                neurons_list[i],",
        "                activation=hp['activation_hidden'],",
        "                kernel_initializer=hp['kernel_initializer'],",
        "                kernel_regularizer=kernel_reg,",
        "                kernel_constraint=kernel_const,",
        "                name=f'layer_{i}'",
        "            ))",
        "        ",
        "        # Batch Normalization",
        "        if hp['use_batch_norm']:",
        "            model.add(BatchNormalization(",
        "                momentum=hp['batch_norm_momentum'],",
        "                epsilon=hp['batch_norm_epsilon']",
        "            ))",
        "        ",
        "        # Dropout",
        "        if dropout_list[i] > 0:",
        "            model.add(Dropout(dropout_list[i]))",
        "    ",
        "    # Capa de salida",
        "    model.add(Dense(",
        "        1,",
        "        activation=hp['activation_output'],",
        "        name='output_layer'",
        "    ))",
        "    ",
        "    # Configurar optimizador",
        "    if hp['optimizer_type'].lower() == 'adam':",
        "        optimizer = tf.keras.optimizers.Adam(",
        "            learning_rate=hp['learning_rate'],",
        "            beta_1=hp['beta_1'],",
        "            beta_2=hp['beta_2'],",
        "            epsilon=hp['epsilon'],",
        "            amsgrad=hp['amsgrad'],",
        "            clipnorm=hp['clipnorm'],",
        "            clipvalue=hp['clipvalue']",
        "        )",
        "    elif hp['optimizer_type'].lower() == 'sgd':",
        "        optimizer = tf.keras.optimizers.SGD(",
        "            learning_rate=hp['learning_rate'],",
        "            momentum=hp['momentum'],",
        "            nesterov=hp['nesterov'],",
        "            clipnorm=hp['clipnorm'],",
        "            clipvalue=hp['clipvalue']",
        "        )",
        "    elif hp['optimizer_type'].lower() == 'nadam':",
        "        optimizer = tf.keras.optimizers.Nadam(",
        "            learning_rate=hp['learning_rate'],",
        "            beta_1=hp['beta_1'],",
        "            beta_2=hp['beta_2'],",
        "            epsilon=hp['epsilon'],",
        "            clipnorm=hp['clipnorm'],",
        "            clipvalue=hp['clipvalue']",
        "        )",
        "    elif hp['optimizer_type'].lower() == 'rmsprop':",
        "        optimizer = tf.keras.optimizers.RMSprop(",
        "            learning_rate=hp['learning_rate'],",
        "            momentum=hp['momentum'],",
        "            epsilon=hp['epsilon'],",
        "            clipnorm=hp['clipnorm'],",
        "            clipvalue=hp['clipvalue']",
        "        )",
        "    else:",
        "        optimizer = Adam(learning_rate=hp['learning_rate'])",
        "    ",
        "    # Configurar m\u00e9tricas",
        "    metrics_list = []",
        "    for metric in hp['metrics']:",
        "        if metric == 'accuracy':",
        "            metrics_list.append('accuracy')",
        "        elif metric == 'auc':",
        "            metrics_list.append(tf.keras.metrics.AUC(name='auc'))",
        "        elif metric == 'precision':",
        "            metrics_list.append(tf.keras.metrics.Precision(name='precision'))",
        "        elif metric == 'recall':",
        "            metrics_list.append(tf.keras.metrics.Recall(name='recall'))",
        "    ",
        "    # Compilar el modelo",
        "    model.compile(",
        "        optimizer=optimizer,",
        "        loss=hp['loss'],",
        "        metrics=metrics_list",
        "    )",
        "    ",
        "    return model",
        "",
        "# Crear el modelo con los hiperpar\u00e1metros",
        "input_dimension = X_train_scaled.shape[1]",
        "model = create_deep_learning_model(input_dimension, hiperparametros)",
        "",
        "# Mostrar la arquitectura del modelo",
        "print(\"Arquitectura del Modelo de Deep Learning:\\n\")",
        "model.summary()"
      ],
      "metadata": {
        "id": "create_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 5. Entrenamiento del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir callbacks basados en hiperpar\u00e1metros",
        "callbacks = []",
        "",
        "if hiperparametros['use_early_stopping']:",
        "    early_stopping = EarlyStopping(",
        "        monitor=hiperparametros['early_stopping_monitor'],",
        "        patience=hiperparametros['early_stopping_patience'],",
        "        min_delta=hiperparametros['early_stopping_min_delta'],",
        "        mode=hiperparametros['early_stopping_mode'],",
        "        restore_best_weights=hiperparametros['restore_best_weights'],",
        "        verbose=hiperparametros['verbose']",
        "    )",
        "    callbacks.append(early_stopping)",
        "",
        "if hiperparametros['use_reduce_lr']:",
        "    reduce_lr = ReduceLROnPlateau(",
        "        monitor=hiperparametros['reduce_lr_monitor'],",
        "        factor=hiperparametros['reduce_lr_factor'],",
        "        patience=hiperparametros['reduce_lr_patience'],",
        "        min_lr=hiperparametros['reduce_lr_min_lr'],",
        "        min_delta=hiperparametros['reduce_lr_min_delta'],",
        "        verbose=hiperparametros['verbose']",
        "    )",
        "    callbacks.append(reduce_lr)",
        "",
        "print(\"Callbacks configurados:\")",
        "if hiperparametros['use_early_stopping']:",
        "    print(f\"- Early Stopping: detiene si no hay mejora en {hiperparametros['early_stopping_patience']} \u00e9pocas\")",
        "if hiperparametros['use_reduce_lr']:",
        "    print(f\"- Reduce LR: reduce learning rate si no hay mejora en {hiperparametros['reduce_lr_patience']} \u00e9pocas\")"
      ],
      "metadata": {
        "id": "callbacks_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo con hiperpar\u00e1metros configurables",
        "print(\"\\nIniciando entrenamiento...\\n\")",
        "",
        "history = model.fit(",
        "    X_train_scaled, y_train,",
        "    validation_split=0.2,",
        "    epochs=hiperparametros['epochs'],",
        "    batch_size=hiperparametros['batch_size'],",
        "    shuffle=hiperparametros['shuffle'],",
        "    callbacks=callbacks,",
        "    verbose=hiperparametros['verbose']",
        ")",
        "",
        "print(\"\\n\u00a1Entrenamiento completado!\")"
      ],
      "metadata": {
        "id": "train_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## 6. Visualizaci\u00f3n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar las curvas de aprendizaje\n",
        "def plot_training_history(history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    axes[0, 0].set_title('P\u00e9rdida durante el Entrenamiento', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('\u00c9poca')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "    axes[0, 1].set_title('Exactitud durante el Entrenamiento', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('\u00c9poca')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # AUC\n",
        "    axes[1, 0].plot(history.history['auc'], label='Training AUC', linewidth=2)\n",
        "    axes[1, 0].plot(history.history['val_auc'], label='Validation AUC', linewidth=2)\n",
        "    axes[1, 0].set_title('AUC durante el Entrenamiento', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('\u00c9poca')\n",
        "    axes[1, 0].set_ylabel('AUC')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Precision y Recall\n",
        "    axes[1, 1].plot(history.history['precision'], label='Training Precision', linewidth=2)\n",
        "    axes[1, 1].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)\n",
        "    axes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2, linestyle='--')\n",
        "    axes[1, 1].plot(history.history['val_recall'], label='Validation Recall', linewidth=2, linestyle='--')\n",
        "    axes[1, 1].set_title('Precision y Recall durante el Entrenamiento', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('\u00c9poca')\n",
        "    axes[1, 1].set_ylabel('M\u00e9trica')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "plot_history_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 7. Evaluaci\u00f3n del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar predicciones con umbral configurable",
        "y_pred_proba = model.predict(X_test_scaled)",
        "y_pred = (y_pred_proba > hiperparametros['prediction_threshold']).astype(int).flatten()",
        "",
        "print(\"Predicciones realizadas en el conjunto de prueba.\")",
        "print(f\"Umbral de predicci\u00f3n: {hiperparametros['prediction_threshold']}\")",
        "print(f\"Forma de y_pred_proba: {y_pred_proba.shape}\")",
        "print(f\"Forma de y_pred: {y_pred.shape}\")"
      ],
      "metadata": {
        "id": "predictions_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular m\u00e9tricas de evaluaci\u00f3n\n",
        "accuracy_dl = accuracy_score(y_test, y_pred)\n",
        "precision_dl = precision_score(y_test, y_pred)\n",
        "recall_dl = recall_score(y_test, y_pred)\n",
        "f1_dl = f1_score(y_test, y_pred)\n",
        "roc_auc_dl = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"M\u00c9TRICAS DE RENDIMIENTO - MODELO DE DEEP LEARNING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {accuracy_dl:.4f} ({accuracy_dl*100:.2f}%)\")\n",
        "print(f\"Precision: {precision_dl:.4f}\")\n",
        "print(f\"Recall:    {recall_dl:.4f}\")\n",
        "print(f\"F1-Score:  {f1_dl:.4f}\")\n",
        "print(f\"AUC-ROC:   {roc_auc_dl:.4f}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "metrics_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reporte de clasificaci\u00f3n detallado\n",
        "print(\"\\nREPORTE DE CLASIFICACI\u00d3N DETALLADO:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No Incumplimiento', 'Incumplimiento']))"
      ],
      "metadata": {
        "id": "classification_report_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusi\u00f3n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Incumplimiento', 'Incumplimiento'],\n",
        "            yticklabels=['No Incumplimiento', 'Incumplimiento'],\n",
        "            cbar_kws={'label': 'Conteo'})\n",
        "plt.title('Matriz de Confusi\u00f3n - Modelo Deep Learning', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.xlabel('Valor Predicho')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretaci\u00f3n de la Matriz de Confusi\u00f3n:\")\n",
        "print(f\"Verdaderos Negativos (TN): {cm[0,0]} - Correctamente clasificados como NO incumplimiento\")\n",
        "print(f\"Falsos Positivos (FP): {cm[0,1]} - Incorrectamente clasificados como incumplimiento\")\n",
        "print(f\"Falsos Negativos (FN): {cm[1,0]} - Incorrectamente clasificados como NO incumplimiento\")\n",
        "print(f\"Verdaderos Positivos (TP): {cm[1,1]} - Correctamente clasificados como incumplimiento\")"
      ],
      "metadata": {
        "id": "confusion_matrix_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "         label=f'Curva ROC (AUC = {roc_auc_dl:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='L\u00ednea Base (AUC = 0.5)')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12)\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12)\n",
        "plt.title('Curva ROC - Modelo Deep Learning', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "roc_curve_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison"
      },
      "source": [
        "## 8. Comparaci\u00f3n de Modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame comparativo con los modelos anteriores\n",
        "# NOTA: Estos valores son del modelo original de Random Forest\n",
        "# Ajusta estos valores si tienes resultados diferentes\n",
        "\n",
        "comparison_data = {\n",
        "    'Modelo': ['Regresi\u00f3n Log\u00edstica', 'Random Forest', 'Deep Learning'],\n",
        "    'Accuracy': [0.6300, 0.6420, accuracy_dl],  # Valores aproximados del modelo original\n",
        "    'Precision': [0.4500, 0.4637, precision_dl],\n",
        "    'Recall': [0.3200, 0.3382, recall_dl],\n",
        "    'F1-Score': [0.3750, 0.3912, f1_dl],\n",
        "    'AUC-ROC': [0.6200, 0.6377, roc_auc_dl]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nCOMPARACI\u00d3N DE MODELOS:\\n\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Identificar el mejor modelo por m\u00e9trica\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MEJOR MODELO POR M\u00c9TRICA:\")\n",
        "print(\"=\"*60)\n",
        "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']:\n",
        "    best_idx = comparison_df[metric].idxmax()\n",
        "    best_model = comparison_df.loc[best_idx, 'Modelo']\n",
        "    best_value = comparison_df.loc[best_idx, metric]\n",
        "    print(f\"{metric:12} : {best_model:20} ({best_value:.4f})\")"
      ],
      "metadata": {
        "id": "comparison_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizaci\u00f3n comparativa\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Gr\u00e1fico de barras agrupadas\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "axes[0].bar(x - width, comparison_df.iloc[0, 1:], width, label='Regresi\u00f3n Log\u00edstica', alpha=0.8)\n",
        "axes[0].bar(x, comparison_df.iloc[1, 1:], width, label='Random Forest', alpha=0.8)\n",
        "axes[0].bar(x + width, comparison_df.iloc[2, 1:], width, label='Deep Learning', alpha=0.8)\n",
        "\n",
        "axes[0].set_xlabel('M\u00e9tricas', fontsize=12)\n",
        "axes[0].set_ylabel('Valor', fontsize=12)\n",
        "axes[0].set_title('Comparaci\u00f3n de M\u00e9tricas entre Modelos', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(metrics, rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "axes[0].set_ylim(0, 1)\n",
        "\n",
        "# Gr\u00e1fico de radar\n",
        "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Cerrar el c\u00edrculo\n",
        "\n",
        "ax = plt.subplot(122, projection='polar')\n",
        "\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    values = row[1:].tolist()\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Modelo'])\n",
        "    ax.fill(angles, values, alpha=0.15)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Comparaci\u00f3n Radial de M\u00e9tricas', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "comparison_viz_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_model"
      },
      "source": [
        "## 9. Guardar el Modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo entrenado\n",
        "model.save('/content/drive/MyDrive/Credit Model/credit_scoring_deep_learning_model.h5')\n",
        "print(\"Modelo guardado exitosamente en: credit_scoring_deep_learning_model.h5\")\n",
        "\n",
        "# Guardar el scaler para uso futuro\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/Credit Model/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"Scaler guardado exitosamente en: scaler.pkl\")\n",
        "\n",
        "# Guardar los label encoders\n",
        "with open('/content/drive/MyDrive/Credit Model/label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "print(\"Label encoders guardados exitosamente en: label_encoders.pkl\")"
      ],
      "metadata": {
        "id": "save_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "## 10. Funci\u00f3n de Predicci\u00f3n para Nuevos Datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_credit_default(new_data, model, scaler, label_encoders):\n",
        "    \"\"\"\n",
        "    Predice el riesgo de incumplimiento para nuevos datos.\n",
        "    \n",
        "    Args:\n",
        "        new_data: DataFrame con las caracter\u00edsticas del nuevo cliente\n",
        "        model: Modelo de Deep Learning entrenado\n",
        "        scaler: StandardScaler ajustado\n",
        "        label_encoders: Diccionario con los LabelEncoders para variables categ\u00f3ricas\n",
        "    \n",
        "    Returns:\n",
        "        Probabilidad de incumplimiento y clasificaci\u00f3n binaria\n",
        "    \"\"\"\n",
        "    # Crear una copia para no modificar los datos originales\n",
        "    data = new_data.copy()\n",
        "    \n",
        "    # Eliminar Customer_ID si existe\n",
        "    if 'Customer_ID' in data.columns:\n",
        "        data = data.drop('Customer_ID', axis=1)\n",
        "    \n",
        "    # Codificar variables categ\u00f3ricas\n",
        "    for col, le in label_encoders.items():\n",
        "        if col in data.columns:\n",
        "            data[col] = le.transform(data[col])\n",
        "    \n",
        "    # Escalar los datos\n",
        "    data_scaled = scaler.transform(data)\n",
        "    \n",
        "    # Realizar la predicci\u00f3n\n",
        "    probability = model.predict(data_scaled)[0][0]\n",
        "    prediction = 1 if probability > 0.5 else 0\n",
        "    \n",
        "    return {\n",
        "        'probabilidad_incumplimiento': probability,\n",
        "        'prediccion': 'Incumplimiento' if prediction == 1 else 'No Incumplimiento',\n",
        "        'score_crediticio': int((1 - probability) * 1000)  # Score de 0-1000\n",
        "    }\n",
        "\n",
        "print(\"Funci\u00f3n de predicci\u00f3n definida.\")\n",
        "print(\"\\nEjemplo de uso:\")\n",
        "print(\"resultado = predict_credit_default(nuevo_cliente_df, model, scaler, label_encoders)\")"
      ],
      "metadata": {
        "id": "inference_function_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de predicci\u00f3n con un cliente del conjunto de prueba\n",
        "sample_index = 0\n",
        "sample_data = X_test.iloc[[sample_index]]\n",
        "\n",
        "# Hacer la predicci\u00f3n\n",
        "result = predict_credit_default(sample_data, model, scaler, label_encoders)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EJEMPLO DE PREDICCI\u00d3N PARA UN NUEVO CLIENTE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Probabilidad de Incumplimiento: {result['probabilidad_incumplimiento']:.4f} ({result['probabilidad_incumplimiento']*100:.2f}%)\")\n",
        "print(f\"Predicci\u00f3n: {result['prediccion']}\")\n",
        "print(f\"Score Crediticio: {result['score_crediticio']}/1000\")\n",
        "print(f\"\\nValor Real: {'Incumplimiento' if y_test.iloc[sample_index] == 1 else 'No Incumplimiento'}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "example_prediction_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusions"
      },
      "source": [
        "## 11. Conclusiones\n",
        "\n",
        "### Resumen del Modelo\n",
        "\n",
        "Este notebook implementa un modelo de Deep Learning para scoring crediticio utilizando una red neuronal profunda con las siguientes caracter\u00edsticas:\n",
        "\n",
        "**Arquitectura:**\n",
        "- Red neuronal con 4 capas ocultas (128 \u2192 64 \u2192 32 \u2192 16 neuronas)\n",
        "- Batch Normalization para estabilizar el entrenamiento\n",
        "- Dropout (30%-20%) para prevenir overfitting\n",
        "- Activaci\u00f3n ReLU en capas ocultas y Sigmoid en la salida\n",
        "\n",
        "**T\u00e9cnicas de Optimizaci\u00f3n:**\n",
        "- Early Stopping para evitar sobreajuste\n",
        "- Reducci\u00f3n adaptativa de learning rate\n",
        "- Optimizador Adam con learning rate inicial de 0.001\n",
        "\n",
        "**Ventajas del Modelo de Deep Learning:**\n",
        "1. Capacidad para capturar relaciones no lineales complejas\n",
        "2. Aprendizaje autom\u00e1tico de features relevantes\n",
        "3. Escalabilidad a grandes vol\u00famenes de datos\n",
        "4. Flexibilidad en la arquitectura\n",
        "\n",
        "**Pr\u00f3ximos Pasos Recomendados:**\n",
        "1. Experimentar con diferentes arquitecturas (m\u00e1s/menos capas)\n",
        "2. Probar diferentes t\u00e9cnicas de regularizaci\u00f3n\n",
        "3. Implementar class weights para manejar el desbalance de clases\n",
        "4. Realizar validaci\u00f3n cruzada para mayor robustez\n",
        "5. An\u00e1lisis de features m\u00e1s importantes\n",
        "6. Optimizaci\u00f3n de hiperpar\u00e1metros con Grid Search o Bayesian Optimization\n",
        "\n",
        "---\n",
        "**Autor:** Claude  \n",
        "**Fecha:** 2025  \n",
        "**Versi\u00f3n:** 1.0\n",
        ""
      ]
    }
  ]
}